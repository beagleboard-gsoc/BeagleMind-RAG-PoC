Metadata-Version: 2.4
Name: beaglemind-cli
Version: 1.0.0
Summary: Intelligent documentation assistant CLI for Beagleboard projects
Author-email: BeagleMind Team <info@beagleboard.org>
License: MIT
Project-URL: Homepage, https://github.com/beagleboard/beaglemind
Project-URL: Repository, https://github.com/beagleboard/beaglemind
Project-URL: Documentation, https://beaglemind.readthedocs.io
Project-URL: Bug Tracker, https://github.com/beagleboard/beaglemind/issues
Keywords: cli,documentation,AI,RAG,beagleboard
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Topic :: Software Development :: Documentation
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: click>=8.0.0
Requires-Dist: rich>=12.0.0
Requires-Dist: gradio>=4.0.0
Requires-Dist: pymilvus>=2.3.0
Requires-Dist: sentence-transformers>=2.2.0
Requires-Dist: langchain>=0.1.0
Requires-Dist: langchain-community>=0.0.10
Requires-Dist: groq>=0.4.0
Requires-Dist: python-dotenv>=1.0.0
Requires-Dist: numpy>=1.21.0
Requires-Dist: requests>=2.25.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: black>=22.0.0; extra == "dev"
Requires-Dist: flake8>=4.0.0; extra == "dev"
Requires-Dist: mypy>=0.910; extra == "dev"

---
title: Beaglemind Rag Poc
emoji: ðŸ‘€
colorFrom: red
colorTo: purple
sdk: gradio
sdk_version: 5.35.0
app_file: app.py
pinned: false
---
Check out the configuration reference at https://huggingface.co/docs/hub/spaces-config-reference

# BeagleMind CLI

An intelligent documentation assistant CLI tool for Beagleboard projects that uses RAG (Retrieval-Augmented Generation) to answer questions about codebases and documentation.

## Features

- **Multi-backend LLM support**: Use both cloud (Groq) and local (Ollama) language models
- **Intelligent search**: Advanced semantic search with reranking and filtering
- **Rich CLI interface**: Beautiful command-line interface with syntax highlighting
- **Persistent configuration**: Save your preferences for seamless usage
- **Source attribution**: Get references to original documentation and code

## Installation

### Development Installation

```bash
# Clone the repository
git clone <repository-url>
cd rag_poc

# Install in development mode
pip install -e .

# Or install dependencies manually
pip install -r requirements.txt
```

### Using pip (when published)

```bash
pip install beaglemind-cli
```

## Quick Start

### 1. Initialize BeagleMind

Before using the CLI, you need to initialize the system and load the knowledge base:

```bash
beaglemind init
```

This will:
- Connect to the Milvus vector database
- Load the document collection
- Set up the QA system

### 2. List Available Models

See what language models are available:

```bash
# List all models
beaglemind list-models

# List models for specific backend
beaglemind list-models --backend groq
beaglemind list-models --backend ollama
```

### 3. Start Chatting

Ask questions about the documentation:

```bash
# Simple question
beaglemind chat -p "How do I configure the BeagleY-AI board?"

# With specific model and backend
beaglemind chat -p "Show me GPIO examples" --backend groq --model llama-3.3-70b-versatile

# With sources shown
beaglemind chat -p "What are the pin configurations?" --sources
```

## CLI Commands

### `beaglemind init`

Initialize the BeagleMind system.

**Options:**
- `--collection, -c`: Collection name to use (default: beaglemind_docs)
- `--force, -f`: Force re-initialization

**Examples:**
```bash
beaglemind init
beaglemind init --collection my_docs
beaglemind init --force
```

### `beaglemind list-models`

List available language models.

**Options:**
- `--backend, -b`: Show models for specific backend (groq/ollama)

**Examples:**
```bash
beaglemind list-models
beaglemind list-models --backend groq
```

### `beaglemind chat`

Chat with BeagleMind using natural language.

**Options:**
- `--prompt, -p`: Your question (required)
- `--backend, -b`: LLM backend (groq/ollama)
- `--model, -m`: Specific model to use
- `--temperature, -t`: Response creativity (0.0-1.0)
- `--strategy, -s`: Search strategy (adaptive/multi_query/context_aware/default)
- `--sources`: Show source references

**Examples:**
```bash
# Basic usage
beaglemind chat -p "How to flash an image to BeagleY-AI?"

# Advanced usage
beaglemind chat \
  -p "Show me Python GPIO examples" \
  --backend groq \
  --model llama-3.3-70b-versatile \
  --temperature 0.2 \
  --strategy adaptive \
  --sources

# Code-focused questions
beaglemind chat -p "How to implement I2C communication?" --sources

# Documentation questions  
beaglemind chat -p "What are the system requirements?" --strategy context_aware
```

## Configuration

BeagleMind stores configuration in `~/.beaglemind_cli_config.json`:

```json
{
  "collection_name": "beaglemind_docs",
  "default_backend": "groq", 
  "default_model": "llama-3.3-70b-versatile",
  "default_temperature": 0.3,
  "initialized": true
}
```

## Environment Setup

### For Groq (Cloud)

Set your Groq API key:

```bash
export GROQ_API_KEY="your-api-key-here"
```

### For OpenAI (Cloud)

Set your OpenAI API key:

```bash
export OPENAI_API_KEY="your-api-key-here"
```

### For Ollama (Local)

1. Install Ollama: https://ollama.ai
2. Pull a supported model:
   ```bash
   ollama pull qwen3:1.7b
   ```
3. Ensure Ollama is running:
   ```bash
   ollama serve
   ```

### Vector Database

Configure Milvus/Zilliz connection in `.env`:

```bash
MILVUS_HOST=your-host
MILVUS_PORT=443
MILVUS_USER=your-username  
MILVUS_PASSWORD=your-password
MILVUS_TOKEN=your-token
MILVUS_URI=your-uri
```

## Available Models

### Groq (Cloud)
- llama-3.3-70b-versatile
- llama-3.1-8b-instant
- gemma2-9b-it
- meta-llama/llama-4-scout-17b-16e-instruct
- meta-llama/llama-4-maverick-17b-128e-instruct

### OpenAI (Cloud)
- gpt-4o
- gpt-4o-mini
- gpt-4-turbo
- gpt-3.5-turbo
- o1-preview
- o1-mini

### Ollama (Local)
- qwen3:1.7b

## Search Strategies

- **adaptive**: Automatically selects best strategy based on question type
- **multi_query**: Uses multiple related queries for comprehensive results
- **context_aware**: Includes surrounding context from documents
- **default**: Standard semantic search

## Tips for Best Results

1. **Be specific**: "How to configure GPIO pins on BeagleY-AI?" vs "GPIO help"

2. **Use technical terms**: Include model names, component names, exact error messages

3. **Ask follow-up questions**: Build on previous responses for deeper understanding

4. **Use --sources**: See exactly where information comes from

5. **Try different strategies**: Some work better for different question types

## Troubleshooting

### "BeagleMind is not initialized"
Run `beaglemind init` first.

### "No API Key" for Groq
Set the GROQ_API_KEY environment variable.

### "No API Key" for OpenAI  
Set the OPENAI_API_KEY environment variable.

### "Service Down" for Ollama  
Ensure Ollama is running: `ollama serve`

### "Model not available"
Check `beaglemind list-models` for available options.

## Development

### Running from Source

```bash
# Make the script executable
chmod +x beaglemind

# Run directly
./beaglemind --help

# Or with Python
python -m src.cli --help
```

### Adding New Models

Edit the model lists in `src/cli.py`:

```python
GROQ_MODELS = [
    "new-model-name",
    # ... existing models
]

OPENAI_MODELS = [
    "new-openai-model",
    # ... existing models
]

OLLAMA_MODELS = [
    "new-local-model",
    # ... existing models  
]
```

## License

MIT License - see LICENSE file for details.

## Support

- GitHub Issues: https://github.com/beagleboard/beaglemind/issues
- Documentation: https://beaglemind.readthedocs.io
- Community: BeagleBoard forums
